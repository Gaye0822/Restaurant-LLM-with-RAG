{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ec7e5aa-b793-4fcb-8c88-7035d3783e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "import openai\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "from typing import Union, List ,Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from typing_extensions import Annotated, TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec53fa7c-2229-498d-99fc-037680faa51d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(r\"C:/Users/helin/source/repos/opengll/gpt/api.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d764ffe9-3426-44db-bd90-3c07733b98ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bf6211-1b8a-41d4-86e7-ee32b86a9fec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# read menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61ca69d0-eee1-45b9-8173-7bf321536c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'Burger', 'description': 'Beef patty with lettuce and tomato', 'ingredients': 'beef,lettuce,tomato,bun', 'price': 8.99}, {'name': 'Pizza', 'description': 'Classic cheese pizza', 'ingredients': 'cheese,tomato sauce,flour', 'price': 12.99}, {'name': 'Pasta', 'description': 'Creamy Alfredo pasta', 'ingredients': 'pasta,cream,parmesan', 'price': 10.99}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Menü dosyasını okuma\n",
    "def read_menu(file_path):\n",
    "    menu = pd.read_csv(file_path)\n",
    "    return menu\n",
    "\n",
    "# Menü verisini yapay zekaya uygun bir formata dönüştürme\n",
    "def prepare_menu_for_gpt(menu):\n",
    "    menu_items = []\n",
    "    for _, row in menu.iterrows():\n",
    "        menu_items.append({\n",
    "            \"name\": row[\"item_name\"],\n",
    "            \"description\": row[\"description\"],\n",
    "            \"ingredients\": row[\"ingredients\"],\n",
    "            \"price\": row[\"price\"]\n",
    "        })\n",
    "    return menu_items\n",
    "\n",
    "# Menü örneği\n",
    "file_path = \"menu.csv\"  # Menü CSV dosyasının yolu\n",
    "menu = read_menu(file_path)\n",
    "menu_for_gpt = prepare_menu_for_gpt(menu)\n",
    "\n",
    "# Menü bilgisi GPT'ye gönderilebilir\n",
    "print(menu_for_gpt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2038cd-814b-40d3-bf2c-64709d01a0b9",
   "metadata": {},
   "source": [
    "# System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "695b72ea-d921-40de-9b05-84eb625fed21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\helin\\AppData\\Local\\Temp\\ipykernel_3820\\1509826299.py:31: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(llm=llm, prompt=prompt)\n"
     ]
    }
   ],
   "source": [
    "system_prompt = (\n",
    "    \"You are a professional waiter at a high-quality restaurant. \"\n",
    "    \"Your job is to politely and concisely answer questions about the menu. \"\n",
    "    \"Always start your response with a warm greeting, such as 'Welcome!' or 'Hello, how can I assist you today?'. \"\n",
    "    \"Use the provided menu context to answer all questions. \"\n",
    "    \"If the item is not available, respond with: 'I’m sorry, that item is not on the menu.' \"\n",
    "    \"If you don't know the answer, say: 'I’m sorry, I’m not sure about that.' \"\n",
    "    \"When confirming an order, repeat it politely and thank the guest. \"\n",
    "    \"Do not recommend items outside the provided menu. \"\n",
    "    \"Structure your responses as follows: \"\n",
    "    \"1. Start with a warm greeting. \"\n",
    "    \"2. Provide a clear and concise answer to the question. \"\n",
    "    \"3. Close with a polite and professional offer to assist further. \"\n",
    "    \"Only response the main question. Don't say anything except the question's response. \"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "# ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Create the question-answer chain\n",
    "#question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2ff1dacd-82f5-41d2-a2f0-f3f8a0c3dad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class sip(TypedDict):\n",
    "    \"\"\"Represents a sentence describing an ordered item.\"\"\"\n",
    "\n",
    "    sip_name: Annotated[str, ..., \"Name of the ordered item, such as a menu item.\"]\n",
    "    sip_count: Annotated[int, ..., \"Quantity of the ordered item.\"]\n",
    "\n",
    "class non_sip(TypedDict):\n",
    "    \"\"\"Represents a sentence describing a non-ordered item.\"\"\"\n",
    "\n",
    "    non_sip_name: Annotated[str, ..., \"Name of the item, without an order (e.g., general reference to a thing or asking questions about it).\"]\n",
    "    non_sip_count: Annotated[int, ..., \"Count of the item, if mentioned, but not part of an order.\"]\n",
    "\n",
    "class del_item(TypedDict):\n",
    "    \"\"\"Represents a sentence describing an item to be deleted from the order.\"\"\"\n",
    "    \n",
    "    del_name: Annotated[str, ..., \"Name of the ordered item that is being canceled or removed.\"]\n",
    "    del_count: Annotated[int, ..., \"Quantity of the item that is being canceled or removed. This value can be positive or indicate the total amount to be canceled.\"]\n",
    "    is_all: Annotated[Optional[bool], \"Indicates if the entire item category (e.g., all 'hamburgers') is being canceled. 'True' means all instances of that item will be canceled, 'False' means just a specific quantity or instance.\"]\n",
    "    description: Annotated[Optional[str], \"Describes the user's intent to cancel all instances of a specific item category, like 'hamburgers', without providing a specific count. This could include phrases like 'I want to cancel all hamburgers'.\"]\n",
    "\n",
    "class FinalResponse(BaseModel):\n",
    "    final_output: List[Union[sip, non_sip, del_item]]\n",
    "\n",
    "# Example of structured output model usage\n",
    "structured_llm = llm.with_structured_output(FinalResponse)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "55d15b69-dda7-47b8-bc8e-38ee88db7e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "system1 = \"\"\"You are an expert at converting user questions into database queries. \\\n",
    "customers will talk you about the menu and you will tke the order or answer their questions \\\n",
    "you have to understand the input for deciding to take an order or answer the question \\\n",
    "if you dont understand the meaning of a sentence or the intent tell it \"\"\"\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system1),\n",
    "        (\"human\", \"{query}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b375f37e-4b21-4511-8bee-b8e2d423b07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_llm1 = llm.with_structured_output(FinalResponse)\n",
    "query_analyzer = prompt1 | structured_llm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d0860e-ee8f-4285-98f3-7d841ca95cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd0404bf-a64b-4250-9aea-e747d33123f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91mError:\u001b[0m Windows is not supported yet in the migration CLI\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
